{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "\n",
    "# Function to generate data in batches\n",
    "def generate_data(batch_size=50):\n",
    "    dataset = []\n",
    "    \n",
    "    \n",
    "    prompt = (\n",
    "      \n",
    "        '''\n",
    "Generate 500 examples in total\n",
    "# Dataset Generation Prompt for QA vs Summarization Classification\n",
    "\n",
    "## Task Description\n",
    "Generate a dataset of user inputs for a conversational chat application that need to be classified into two categories:\n",
    "1. **QA (Question Answering)**: Queries where the user is asking a specific question and expects a direct answer\n",
    "2. **Summarization**: Requests where the user wants content or information to be condensed or summarized\n",
    "\n",
    "## Dataset Requirements\n",
    "- Generate 500 examples in total:\n",
    "  - 250 examples of QA queries\n",
    "  - 250 examples of summarization requests\n",
    "- Each example should include:\n",
    "  - The user input text\n",
    "  - The correct classification label (QA or Summarization)\n",
    "  - A brief explanation of why this classification is appropriate (this will help with verification)\n",
    "- The examples should cover a wide range of topics, domains, and complexity levels\n",
    "- Include variations in query structure, length, and formality\n",
    "- Include edge cases and ambiguous queries\n",
    "\n",
    "## Output Format\n",
    "Please provide the dataset in JSON format:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"input\": \"What is the capital of France?\",\n",
    "    \"classification\": \"QA\",\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"Can you summarize the key points from the attached meeting transcript?\",\n",
    "    \"classification\": \"Summarization\",\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "## Guidelines for Creating Diverse Examples\n",
    "\n",
    "### QA Examples Should Include:\n",
    "- Factual questions \n",
    "- How-to questions \n",
    "- Why/reason questions \n",
    "- Comparison questions \n",
    "- Opinion/recommendation requests \n",
    "- Hypothetical questions \n",
    "- Clarification questions \n",
    "- Yes/no questions \n",
    "\n",
    "### Summarization Examples Should Include:\n",
    "- Requests to summarize articles, papers, or documents\n",
    "- Requests to condense meeting notes or transcripts\n",
    "- Requests to extract key points from longer content\n",
    "- Requests to simplify complex information\n",
    "- Requests to provide overviews of topics or concepts\n",
    "- Requests to create executive summaries\n",
    "- Requests to distill main arguments from discussions\n",
    "- Requests to create bullet-point summaries\n",
    "\n",
    "### Edge Cases to Include:\n",
    "- Ambiguous queries that could be either QA or summarization based on context\n",
    "- Multi-part queries with both QA and summarization elements\n",
    "- Very short or very long queries\n",
    "- Poorly formulated or grammatically incorrect queries\n",
    "- Queries with jargon or technical language\n",
    "- Conversational or informal queries\n",
    "\n",
    "## Ensure Diversity In:\n",
    "- Topics (technology, science, business, entertainment, politics, history, etc.)\n",
    "- Complexity levels (basic to advanced)\n",
    "- Query length (short, medium, long)\n",
    "- Formality (casual to formal language)\n",
    "- Specificity (very specific to broad queries)\n",
    "'''\n",
    "    \n",
    "    )\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",  # Use gpt-3.5-turbo if needed\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset generation...\n",
      "Generating QA batch 1...\n",
      "Added 20 QA examples. Total QA: 20/250\n",
      "Generating QA batch 2...\n",
      "Added 20 QA examples. Total QA: 40/250\n",
      "Generating QA batch 3...\n",
      "Added 20 QA examples. Total QA: 60/250\n",
      "Generating QA batch 4...\n",
      "Added 20 QA examples. Total QA: 80/250\n",
      "Generating QA batch 5...\n",
      "Added 20 QA examples. Total QA: 100/250\n",
      "Generating QA batch 6...\n",
      "Added 20 QA examples. Total QA: 120/250\n",
      "Generating QA batch 7...\n",
      "Added 20 QA examples. Total QA: 140/250\n",
      "Generating QA batch 8...\n",
      "Added 20 QA examples. Total QA: 160/250\n",
      "Generating QA batch 9...\n",
      "Added 20 QA examples. Total QA: 180/250\n",
      "Generating QA batch 10...\n",
      "Added 20 QA examples. Total QA: 200/250\n",
      "Generating QA batch 11...\n",
      "Added 20 QA examples. Total QA: 220/250\n",
      "Generating QA batch 12...\n",
      "Added 20 QA examples. Total QA: 240/250\n",
      "Generating QA batch 13...\n",
      "Added 10 QA examples. Total QA: 250/250\n",
      "Generating Summarization batch 1...\n",
      "Added 20 Summarization examples. Total Summarization: 20/250\n",
      "Generating Summarization batch 2...\n",
      "Added 20 Summarization examples. Total Summarization: 40/250\n",
      "Generating Summarization batch 3...\n",
      "Added 20 Summarization examples. Total Summarization: 60/250\n",
      "Generating Summarization batch 4...\n",
      "Added 20 Summarization examples. Total Summarization: 80/250\n",
      "Generating Summarization batch 5...\n",
      "Added 20 Summarization examples. Total Summarization: 100/250\n",
      "Generating Summarization batch 6...\n",
      "Added 20 Summarization examples. Total Summarization: 120/250\n",
      "Generating Summarization batch 7...\n",
      "Added 20 Summarization examples. Total Summarization: 140/250\n",
      "Generating Summarization batch 8...\n",
      "Added 20 Summarization examples. Total Summarization: 160/250\n",
      "Generating Summarization batch 9...\n",
      "Added 20 Summarization examples. Total Summarization: 180/250\n",
      "Generating Summarization batch 10...\n",
      "Added 20 Summarization examples. Total Summarization: 200/250\n",
      "Generating Summarization batch 11...\n",
      "Added 20 Summarization examples. Total Summarization: 220/250\n",
      "Generating Summarization batch 12...\n",
      "Added 20 Summarization examples. Total Summarization: 240/250\n",
      "Generating Summarization batch 13...\n",
      "Added 10 Summarization examples. Total Summarization: 250/250\n",
      "Dataset generation complete. Total examples: 500\n",
      "Dataset saved to qa_summarization_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Function to generate data in smaller batches\n",
    "def generate_data_batch(batch_size=20, category=None, batch_number=1):\n",
    "    # Create a focused prompt for each batch\n",
    "    base_prompt = \"\"\"\n",
    "        Generate exactly {0} examples of user inputs for a conversational chat application that need to be classified as {1}.\n",
    "\n",
    "        The examples should be diverse across topics, complexity levels, query length, and formality.\n",
    "        Each example should include:\n",
    "        - The user input text\n",
    "        - The classification label (\"{1}\")\n",
    "\n",
    "        Output ONLY valid JSON format like this:\n",
    "        [\n",
    "        {{\n",
    "            \"input\": \"example text here\",\n",
    "            \"classification\": \"{1}\"\n",
    "        }},\n",
    "        ...\n",
    "        ]\n",
    "\n",
    "        Make sure:\n",
    "        1. The JSON is properly formatted\n",
    "        2. There are exactly {0} examples\n",
    "        3. All examples are clearly {1} requests\n",
    "        \"\"\"\n",
    "    \n",
    "    # Customize the prompt based on the category\n",
    "    if category == \"QA\":\n",
    "        category_guidance = \"\"\"\n",
    "        Include a mix of:\n",
    "        - Factual questions\n",
    "        - How-to questions\n",
    "        - Why/reason questions\n",
    "        - Comparison questions\n",
    "        - Opinion/recommendation requests\n",
    "        - Hypothetical questions\n",
    "        - Clarification questions\n",
    "        - Yes/no questions\n",
    "        - Different complexity levels from simple to complex\n",
    "        - Batch #{0}: focus on {1}\n",
    "        \"\"\"\n",
    "        # Rotate focus areas for better diversity\n",
    "        focus_areas = [\"technology\", \"science\", \"business\", \"everyday life\", \"history\", \"entertainment\"]\n",
    "        focus_area = focus_areas[batch_number % len(focus_areas)]\n",
    "        \n",
    "    else:  # Summarization\n",
    "        category_guidance = \"\"\"\n",
    "        Include a mix of:\n",
    "        - Requests to summarize articles, papers, or documents\n",
    "        - Requests to condense meeting notes or transcripts\n",
    "        - Requests to extract key points from longer content\n",
    "        - Requests to simplify complex information\n",
    "        - Requests to provide overviews of topics or concepts\n",
    "        - Requests to create executive summaries\n",
    "        - Requests to distill main arguments from discussions\n",
    "        - Requests to create bullet-point summaries\n",
    "        - Batch #{0}: focus on {1}\n",
    "        \"\"\"\n",
    "        # Rotate focus areas for better diversity\n",
    "        focus_areas = [\"business documents\", \"academic papers\", \"news articles\", \"technical content\", \"meeting notes\", \"general information\"]\n",
    "        focus_area = focus_areas[batch_number % len(focus_areas)]\n",
    "    \n",
    "    # Complete the prompt with category-specific guidance\n",
    "    full_prompt = base_prompt.format(batch_size, category) + category_guidance.format(batch_number, focus_area)\n",
    "    \n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",  # You can use \"gpt-3.5-turbo\" for faster/cheaper generation\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert data generator for NLP tasks.\"},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "        \n",
    "        # Extract the JSON string\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Find the JSON portion (between square brackets)\n",
    "        json_start = response_text.find('[')\n",
    "        json_end = response_text.rfind(']') + 1\n",
    "        \n",
    "        if json_start != -1 and json_end != -1:\n",
    "            json_string = response_text[json_start:json_end]\n",
    "            try:\n",
    "                data = json.loads(json_string)\n",
    "                return data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON parsing error in batch {batch_number}: {str(e)}\")\n",
    "                print(\"Problematic JSON string:\", json_string[:100] + \"...\" if len(json_string) > 100 else json_string)\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"Failed to extract JSON from response for batch {batch_number}\")\n",
    "            print(\"Response:\", response_text[:100] + \"...\" if len(response_text) > 100 else response_text)\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating batch {batch_number}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Main function to generate the full dataset\n",
    "def generate_full_dataset(total_examples=500):\n",
    "    dataset = []\n",
    "    examples_per_category = total_examples // 2\n",
    "    batch_size = 20  # Smaller batch size for more reliable generation\n",
    "    \n",
    "    # Generate QA examples\n",
    "    qa_count = 0\n",
    "    batch_number = 1\n",
    "    while qa_count < examples_per_category:\n",
    "        print(f\"Generating QA batch {batch_number}...\")\n",
    "        current_batch_size = min(batch_size, examples_per_category - qa_count)\n",
    "        batch_data = generate_data_batch(batch_size=current_batch_size, category=\"QA\", batch_number=batch_number)\n",
    "        \n",
    "        if batch_data:\n",
    "            dataset.extend(batch_data)\n",
    "            qa_count += len(batch_data)\n",
    "            print(f\"Added {len(batch_data)} QA examples. Total QA: {qa_count}/{examples_per_category}\")\n",
    "        \n",
    "        batch_number += 1\n",
    "        time.sleep(3)  # Add delay to avoid rate limits\n",
    "    \n",
    "    # Generate Summarization examples\n",
    "    summarization_count = 0\n",
    "    batch_number = 1\n",
    "    while summarization_count < examples_per_category:\n",
    "        print(f\"Generating Summarization batch {batch_number}...\")\n",
    "        current_batch_size = min(batch_size, examples_per_category - summarization_count)\n",
    "        batch_data = generate_data_batch(batch_size=current_batch_size, category=\"Summarization\", batch_number=batch_number)\n",
    "        \n",
    "        if batch_data:\n",
    "            dataset.extend(batch_data)\n",
    "            summarization_count += len(batch_data)\n",
    "            print(f\"Added {len(batch_data)} Summarization examples. Total Summarization: {summarization_count}/{examples_per_category}\")\n",
    "        \n",
    "        batch_number += 1\n",
    "        time.sleep(3)  # Add delay to avoid rate limits\n",
    "    \n",
    "    # Shuffle the dataset for better training\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Generate and save the dataset\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting dataset generation...\")\n",
    "    dataset = generate_full_dataset(total_examples=500)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"qa_summarization_dataset.json\", \"w\") as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset generation complete. Total examples: {len(dataset)}\")\n",
    "    print(f\"Dataset saved to qa_summarization_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
