{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:01.654756Z",
     "iopub.status.busy": "2025-03-24T20:50:01.654537Z",
     "iopub.status.idle": "2025-03-24T20:50:01.659016Z",
     "shell.execute_reply": "2025-03-24T20:50:01.658162Z",
     "shell.execute_reply.started": "2025-03-24T20:50:01.654738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:31.635352Z",
     "iopub.status.busy": "2025-03-24T20:50:31.634985Z",
     "iopub.status.idle": "2025-03-24T20:50:31.645661Z",
     "shell.execute_reply": "2025-03-24T20:50:31.644698Z",
     "shell.execute_reply.started": "2025-03-24T20:50:31.635321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Dataset Handling\n",
    "class PronounResolutionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Get text with marked pronoun and candidates\n",
    "        text = item['text'].lower()\n",
    "        pronoun = item['pronoun'].lower()\n",
    "        candidates = [candidate.lower() for candidate in item['candidates']]\n",
    "        pronoun_position = item['pronoun_position']\n",
    "        correct_candidate_idx = item['correct_candidate_idx']\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get pronoun token position in BERT tokenization\n",
    "        pronoun_token_position = None\n",
    "        input_ids = encoding['input_ids'][0]\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        # Find the pronoun position in tokenized text\n",
    "        # This is approximate and may need refinement based on dataset specifics\n",
    "        text_until_pronoun = text[:pronoun_position]\n",
    "        approx_token_count = len(self.tokenizer.tokenize(text_until_pronoun))\n",
    "        pronoun_token_position = approx_token_count\n",
    "        \n",
    "        # Tokenize candidates\n",
    "        candidate_encodings = []\n",
    "        for candidate in candidates:\n",
    "            candidate_encoding = self.tokenizer(\n",
    "                candidate,\n",
    "                max_length=20,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            candidate_encodings.append({\n",
    "                'input_ids': candidate_encoding['input_ids'][0],\n",
    "                'attention_mask': candidate_encoding['attention_mask'][0]\n",
    "            })\n",
    "        \n",
    "        # Pad candidate list if needed\n",
    "        max_candidates = 5  # Adjust based on your dataset\n",
    "        while len(candidate_encodings) < max_candidates:\n",
    "            # Add padding candidate\n",
    "            pad_encoding = self.tokenizer(\n",
    "                \"\",\n",
    "                max_length=20,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            candidate_encodings.append({\n",
    "                'input_ids': pad_encoding['input_ids'][0],\n",
    "                'attention_mask': pad_encoding['attention_mask'][0]\n",
    "            })\n",
    "        \n",
    "        # Convert candidate encodings to tensors\n",
    "        candidate_input_ids = torch.stack([c['input_ids'] for c in candidate_encodings[:max_candidates]])\n",
    "        candidate_attention_masks = torch.stack([c['attention_mask'] for c in candidate_encodings[:max_candidates]])\n",
    "        \n",
    "        # If we have more candidates than our max, truncate\n",
    "        if len(candidates) > max_candidates:\n",
    "            candidate_input_ids = candidate_input_ids[:max_candidates]\n",
    "            candidate_attention_masks = candidate_attention_masks[:max_candidates]\n",
    "            if correct_candidate_idx >= max_candidates:\n",
    "                correct_candidate_idx = max_candidates - 1  # Adjust if needed\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'pronoun_position': torch.tensor(pronoun_token_position, dtype=torch.long),\n",
    "            'candidate_input_ids': candidate_input_ids,\n",
    "            'candidate_attention_masks': candidate_attention_masks,\n",
    "            'correct_candidate_idx': torch.tensor(correct_candidate_idx, dtype=torch.long),\n",
    "            'num_candidates': torch.tensor(min(len(candidates), max_candidates), dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:34.099710Z",
     "iopub.status.busy": "2025-03-24T20:50:34.099415Z",
     "iopub.status.idle": "2025-03-24T20:50:34.104474Z",
     "shell.execute_reply": "2025-03-24T20:50:34.103506Z",
     "shell.execute_reply.started": "2025-03-24T20:50:34.099689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to dataset\n",
    "def load_data(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # validation\n",
    "    for item in data:\n",
    "        assert 'text' in item, \"Missing 'text' field\"\n",
    "        assert 'pronoun' in item, \"Missing 'pronoun' field\"\n",
    "        assert 'candidates' in item, \"Missing 'candidates' field\"\n",
    "        assert 'pronoun_position' in item, \"Missing 'pronoun_position' field\"\n",
    "        assert 'correct_candidate_idx' in item, \"Missing 'correct_candidate_idx' field\"\n",
    "    \n",
    "    random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:36.969027Z",
     "iopub.status.busy": "2025-03-24T20:50:36.968718Z",
     "iopub.status.idle": "2025-03-24T20:50:36.978080Z",
     "shell.execute_reply": "2025-03-24T20:50:36.977049Z",
     "shell.execute_reply.started": "2025-03-24T20:50:36.969003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class PronounResolutionModel(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', dropout_rate=0.1):\n",
    "        super(PronounResolutionModel, self).__init__()\n",
    "        \n",
    "        # BERT for contextualized embeddings\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Candidate encoder\n",
    "        self.candidate_bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Attention mechanism for candidate scoring\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 2, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # Additional layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pronoun_position, \n",
    "                candidate_input_ids, candidate_attention_masks, num_candidates):\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Get contextualized embeddings from BERT\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Extract pronoun representations\n",
    "        pronoun_representations = []\n",
    "        for i in range(batch_size):\n",
    "            pos = pronoun_position[i]\n",
    "            pronoun_representations.append(sequence_output[i, pos])\n",
    "        \n",
    "        pronoun_representations = torch.stack(pronoun_representations)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Process candidates\n",
    "        max_candidates = candidate_input_ids.size(1)\n",
    "        candidate_representations = []\n",
    "        \n",
    "        # Reshape for batch processing\n",
    "        flat_candidate_ids = candidate_input_ids.view(-1, candidate_input_ids.size(-1))\n",
    "        flat_candidate_masks = candidate_attention_masks.view(-1, candidate_attention_masks.size(-1))\n",
    "        \n",
    "        # Get candidate embeddings\n",
    "        candidate_outputs = self.candidate_bert(\n",
    "            input_ids=flat_candidate_ids,\n",
    "            attention_mask=flat_candidate_masks,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token as candidate representation\n",
    "        flat_candidate_embeds = candidate_outputs.last_hidden_state[:, 0]  # [batch_size*max_candidates, hidden_size]\n",
    "        \n",
    "        # Reshape back to [batch_size, max_candidates, hidden_size]\n",
    "        candidate_embeds = flat_candidate_embeds.view(batch_size, max_candidates, -1)\n",
    "        \n",
    "        # Score each candidate\n",
    "        scores = []\n",
    "        for i in range(batch_size):\n",
    "            n_cand = num_candidates[i].item()\n",
    "            \n",
    "            # Expand pronoun representation for each candidate\n",
    "            expanded_pronoun = pronoun_representations[i].unsqueeze(0).expand(n_cand, -1)  # [n_cand, hidden_size]\n",
    "            \n",
    "            # Concatenate pronoun and candidate representations\n",
    "            concat_reps = torch.cat([\n",
    "                expanded_pronoun, \n",
    "                candidate_embeds[i, :n_cand]\n",
    "            ], dim=1)  # [n_cand, hidden_size*2]\n",
    "            \n",
    "            # Score each candidate\n",
    "            cand_scores = self.attention(concat_reps).squeeze(-1)  # [n_cand]\n",
    "            \n",
    "            # Pad scores for batch processing\n",
    "            padded_scores = torch.full((max_candidates,), float('-inf'), device=cand_scores.device)\n",
    "            padded_scores[:n_cand] = cand_scores\n",
    "            scores.append(padded_scores)\n",
    "        \n",
    "        scores = torch.stack(scores)  # [batch_size, max_candidates]\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:39.255762Z",
     "iopub.status.busy": "2025-03-24T20:50:39.255450Z",
     "iopub.status.idle": "2025-03-24T20:50:39.263545Z",
     "shell.execute_reply": "2025-03-24T20:50:39.262482Z",
     "shell.execute_reply.started": "2025-03-24T20:50:39.255735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, \n",
    "                learning_rate=1e-5, epochs=3, warmup_steps=0):\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pronoun_position = batch['pronoun_position'].to(device)\n",
    "            candidate_input_ids = batch['candidate_input_ids'].to(device)\n",
    "            candidate_attention_masks = batch['candidate_attention_masks'].to(device)\n",
    "            correct_candidate_idx = batch['correct_candidate_idx'].to(device)\n",
    "            num_candidates = batch['num_candidates'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pronoun_position=pronoun_position,\n",
    "                candidate_input_ids=candidate_input_ids,\n",
    "                candidate_attention_masks=candidate_attention_masks,\n",
    "                num_candidates=num_candidates\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, correct_candidate_idx)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': train_loss / (progress_bar.n + 1)})\n",
    "        \n",
    "        # Validation\n",
    "        val_acc = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_pronoun_resolution_model.pt')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:41.550576Z",
     "iopub.status.busy": "2025-03-24T20:50:41.550288Z",
     "iopub.status.idle": "2025-03-24T20:50:41.556661Z",
     "shell.execute_reply": "2025-03-24T20:50:41.555707Z",
     "shell.execute_reply.started": "2025-03-24T20:50:41.550556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pronoun_position = batch['pronoun_position'].to(device)\n",
    "            candidate_input_ids = batch['candidate_input_ids'].to(device)\n",
    "            candidate_attention_masks = batch['candidate_attention_masks'].to(device)\n",
    "            correct_candidate_idx = batch['correct_candidate_idx'].to(device)\n",
    "            num_candidates = batch['num_candidates'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pronoun_position=pronoun_position,\n",
    "                candidate_input_ids=candidate_input_ids,\n",
    "                candidate_attention_masks=candidate_attention_masks,\n",
    "                num_candidates=num_candidates\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += correct_candidate_idx.size(0)\n",
    "            correct += (predicted == correct_candidate_idx).sum().item()\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:44.779380Z",
     "iopub.status.busy": "2025-03-24T20:50:44.778909Z",
     "iopub.status.idle": "2025-03-24T20:50:44.790560Z",
     "shell.execute_reply": "2025-03-24T20:50:44.789384Z",
     "shell.execute_reply.started": "2025-03-24T20:50:44.779330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inference Function\n",
    "def resolve_pronoun(model, text, pronoun, pronoun_position, candidates, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input data\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Find pronoun token position\n",
    "    text_until_pronoun = text[:pronoun_position]\n",
    "    approx_token_count = len(tokenizer.tokenize(text_until_pronoun))\n",
    "    pronoun_token_position = torch.tensor([approx_token_count], dtype=torch.long)\n",
    "    \n",
    "    # Tokenize candidates\n",
    "    candidate_encodings = []\n",
    "    for candidate in candidates:\n",
    "        candidate_encoding = tokenizer(\n",
    "            candidate,\n",
    "            max_length=20,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        candidate_encodings.append({\n",
    "            'input_ids': candidate_encoding['input_ids'][0],\n",
    "            'attention_mask': candidate_encoding['attention_mask'][0]\n",
    "        })\n",
    "    \n",
    "    candidate_input_ids = torch.stack([c['input_ids'] for c in candidate_encodings])\n",
    "    candidate_attention_masks = torch.stack([c['attention_mask'] for c in candidate_encodings])\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    pronoun_token_position = pronoun_token_position.to(device)\n",
    "    candidate_input_ids = candidate_input_ids.unsqueeze(0).to(device)\n",
    "    candidate_attention_masks = candidate_attention_masks.unsqueeze(0).to(device)\n",
    "    num_candidates = torch.tensor([len(candidates)], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pronoun_position=pronoun_token_position,\n",
    "            candidate_input_ids=candidate_input_ids,\n",
    "            candidate_attention_masks=candidate_attention_masks,\n",
    "            num_candidates=num_candidates\n",
    "        )\n",
    "    \n",
    "    # Get prediction\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    return candidates[predicted.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:54.290940Z",
     "iopub.status.busy": "2025-03-24T20:50:54.290602Z",
     "iopub.status.idle": "2025-03-24T20:50:54.298652Z",
     "shell.execute_reply": "2025-03-24T20:50:54.297644Z",
     "shell.execute_reply.started": "2025-03-24T20:50:54.290911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    data_path = \"/kaggle/input/augmenteddataest/augmented_pronoun_resolution_data.json\"\n",
    "    \n",
    "    # Load data\n",
    "    data = load_data(data_path)\n",
    "    print(f\"Loaded {len(data)} examples\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print(f\"Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PronounResolutionDataset(train_data, tokenizer)\n",
    "    val_dataset = PronounResolutionDataset(val_data, tokenizer)\n",
    "    test_dataset = PronounResolutionDataset(test_data, tokenizer)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PronounResolutionModel(bert_model_name='bert-base-uncased')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, device, learning_rate=2e-5, epochs=8)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_pronoun_resolution_model.pt'))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_acc = evaluate_model(model, test_loader, device)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    example_text = \"John met Mike at the park. He was happy to see him.\"\n",
    "    pronoun = \"He\"\n",
    "    pronoun_position = 24\n",
    "    candidates = [\"John\", \"Mike\"]\n",
    "    \n",
    "    resolved_entity = resolve_pronoun(\n",
    "        model, example_text, pronoun, pronoun_position, candidates, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    print(f\"Resolved pronoun '{pronoun}' to: {resolved_entity}\")\n",
    "    \n",
    "    # Save model and configuration\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'bert_model_name': 'bert-base-uncased',\n",
    "        'max_length': 128\n",
    "    }, 'pronoun_resolution_model_full.pt')\n",
    "    \n",
    "    print(\"Model saved as 'pronoun_resolution_model_full.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T20:50:55.750268Z",
     "iopub.status.busy": "2025-03-24T20:50:55.749886Z",
     "iopub.status.idle": "2025-03-24T21:13:43.698971Z",
     "shell.execute_reply": "2025-03-24T21:13:43.697704Z",
     "shell.execute_reply.started": "2025-03-24T20:50:55.750236Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 5000 examples\n",
      "Train: 3600, Validation: 400, Test: 1000\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|██████████| 225/225 [02:41<00:00,  1.39it/s, loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Train Loss: 0.7324, Val Accuracy: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|██████████| 225/225 [02:40<00:00,  1.40it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8, Train Loss: 0.3427, Val Accuracy: 0.9175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|██████████| 225/225 [02:40<00:00,  1.40it/s, loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8, Train Loss: 0.2499, Val Accuracy: 0.9125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|██████████| 225/225 [02:40<00:00,  1.40it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8, Train Loss: 0.2122, Val Accuracy: 0.9325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|██████████| 225/225 [02:40<00:00,  1.40it/s, loss=0.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8, Train Loss: 0.1604, Val Accuracy: 0.9450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|██████████| 225/225 [02:40<00:00,  1.40it/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8, Train Loss: 0.1542, Val Accuracy: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|██████████| 225/225 [02:40<00:00,  1.40it/s, loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8, Train Loss: 0.1309, Val Accuracy: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|██████████| 225/225 [02:40<00:00,  1.40it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8, Train Loss: 0.1191, Val Accuracy: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-812eb7137d84>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_pronoun_resolution_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9360\n",
      "Resolved pronoun 'He' to: John\n",
      "Model saved as 'pronoun_resolution_model_full.pt'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T21:15:57.674003Z",
     "iopub.status.busy": "2025-03-24T21:15:57.673655Z",
     "iopub.status.idle": "2025-03-24T21:16:00.667672Z",
     "shell.execute_reply": "2025-03-24T21:16:00.666801Z",
     "shell.execute_reply.started": "2025-03-24T21:15:57.673974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-cd1428ce5c22>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronoun 'it' most likely refers to: egypt\n",
      "Confidence: 0.98\n",
      "\n",
      "All candidates:\n",
      "- egypt: 0.98\n",
      "- what places: 0.02\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def predict_pronoun_resolution(text, pronoun, model_path='pronoun_resolution_model_full.pt', device=None):\n",
    "    \n",
    "    # Determine device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    bert_model_name = checkpoint.get('bert_model_name', 'bert-base-uncased')\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    \n",
    "    # Load model\n",
    "    model = PronounResolutionModel(bert_model_name=bert_model_name)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Find pronoun position in text\n",
    "    pronoun_pattern = re.compile(r'\\b' + re.escape(pronoun) + r'\\b', re.IGNORECASE)\n",
    "    matches = list(pronoun_pattern.finditer(text))\n",
    "    \n",
    "    if not matches:\n",
    "        return {\"error\": f\"Pronoun '{pronoun}' not found in the text\"}\n",
    "    \n",
    "    # For simplicity, we'll use the first occurrence\n",
    "    pronoun_position = matches[0].start()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract named entities and noun chunks as candidates\n",
    "    candidates = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]:  # People, organizations, locations\n",
    "            candidates.append(ent.text)\n",
    "    \n",
    "    # Add noun chunks that might be potential candidates\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # Skip pronouns and determiners\n",
    "        if chunk.root.pos_ not in [\"PRON\", \"DET\"] and chunk.text.lower() != pronoun.lower():\n",
    "            candidates.append(chunk.text)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    candidates = list(dict.fromkeys(candidates))\n",
    "    \n",
    "\n",
    "    if not candidates:\n",
    "        return {\"error\": \"No potential candidate entities found in the text\"}\n",
    "    \n",
    "    # Prepare input data similar to resolve_pronoun function\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Find pronoun token position\n",
    "    text_until_pronoun = text[:pronoun_position]\n",
    "    approx_token_count = len(tokenizer.tokenize(text_until_pronoun))\n",
    "    pronoun_token_position = torch.tensor([approx_token_count], dtype=torch.long)\n",
    "    \n",
    "    # Tokenize candidates\n",
    "    candidate_encodings = []\n",
    "    for candidate in candidates:\n",
    "        candidate_encoding = tokenizer(\n",
    "            candidate,\n",
    "            max_length=20,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        candidate_encodings.append({\n",
    "            'input_ids': candidate_encoding['input_ids'][0],\n",
    "            'attention_mask': candidate_encoding['attention_mask'][0]\n",
    "        })\n",
    "    \n",
    "    candidate_input_ids = torch.stack([c['input_ids'] for c in candidate_encodings])\n",
    "    candidate_attention_masks = torch.stack([c['attention_mask'] for c in candidate_encodings])\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    pronoun_token_position = pronoun_token_position.to(device)\n",
    "    candidate_input_ids = candidate_input_ids.unsqueeze(0).to(device)\n",
    "    candidate_attention_masks = candidate_attention_masks.unsqueeze(0).to(device)\n",
    "    num_candidates = torch.tensor([len(candidates)], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pronoun_position=pronoun_token_position,\n",
    "            candidate_input_ids=candidate_input_ids,\n",
    "            candidate_attention_masks=candidate_attention_masks,\n",
    "            num_candidates=num_candidates\n",
    "        )\n",
    "    \n",
    "    # Get prediction and confidence scores\n",
    "    scores = outputs[0].cpu().detach().numpy()\n",
    "    probabilities = torch.softmax(outputs[0], dim=0).cpu().detach().numpy()\n",
    "    predicted_idx = int(torch.argmax(outputs, dim=1).item())\n",
    "    \n",
    "    # Prepare results\n",
    "    result = {\n",
    "        \"resolved_entity\": candidates[predicted_idx],\n",
    "        \"confidence\": float(probabilities[predicted_idx]),\n",
    "        \"pronoun\": pronoun,\n",
    "        \"pronoun_position\": pronoun_position,\n",
    "        \"candidates\": [\n",
    "            {\"entity\": cand, \"score\": float(score), \"probability\": float(prob)}\n",
    "            for cand, score, prob in zip(candidates, scores, probabilities)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"tell me about egypt. what places can I visit in it\"\n",
    "    pronoun = \"it\"\n",
    "    \n",
    "    result = predict_pronoun_resolution(text, pronoun)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Pronoun '{result['pronoun']}' most likely refers to: {result['resolved_entity']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "        print(\"\\nAll candidates:\")\n",
    "        for candidate in result['candidates']:\n",
    "            print(f\"- {candidate['entity']}: {candidate['probability']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6955275,
     "sourceId": 11148626,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6955513,
     "sourceId": 11148920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6958780,
     "sourceId": 11153370,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
