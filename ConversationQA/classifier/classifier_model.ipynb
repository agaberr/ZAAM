{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import joblib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_file_path):\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return pd.DataFrame(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{json_file_path}' not found.\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "def extract_features(text_series):\n",
    "    features = pd.DataFrame(index=text_series.index)  # Ensure indices match\n",
    "    \n",
    "    # 1. Basic text features\n",
    "    features['text_length'] = text_series.apply(len)\n",
    "    features['word_count'] = text_series.apply(lambda x: len(x.split()))\n",
    "    features['avg_word_length'] = text_series.apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    \n",
    "    # 2. Question-related features\n",
    "    features['has_question_mark'] = text_series.apply(lambda x: 1 if '?' in x else 0)\n",
    "    \n",
    "    # 3. Check for question words\n",
    "    question_words = ['what', 'who', 'when', 'where', 'why', 'how', 'which', 'whose']\n",
    "    for word in question_words:\n",
    "        features[f'starts_with_{word}'] = text_series.apply(\n",
    "            lambda x: 1 if x.lower().strip().startswith(word) else 0\n",
    "        )\n",
    "    \n",
    "    features['starts_with_question_word'] = features[[f'starts_with_{word}' for word in question_words]].max(axis=1)\n",
    "    \n",
    "    # 4. Check for question-asking verbs\n",
    "    question_verbs = ['can', 'could', 'would', 'will', 'should', 'is', 'are', 'do', 'does', 'did']\n",
    "    for verb in question_verbs:\n",
    "        features[f'starts_with_{verb}'] = text_series.apply(\n",
    "            lambda x: 1 if x.lower().strip().startswith(verb) else 0\n",
    "        )\n",
    "    \n",
    "    features['starts_with_question_verb'] = features[[f'starts_with_{verb}' for verb in question_verbs]].max(axis=1)\n",
    "    \n",
    "    # 5. Check for summarization keywords\n",
    "    summarization_words = ['summarize', 'summary', 'summarization', 'condense', 'shorten', \n",
    "                          'brief', 'overview', 'digest', 'recap', 'synopsis', 'tldr', \n",
    "                          'key points', 'main points', 'highlight', 'gist', 'bullet']\n",
    "    \n",
    "    features['contains_summarization_word'] = text_series.apply(\n",
    "        lambda x: 1 if any(word in x.lower() for word in summarization_words) else 0\n",
    "    )\n",
    "    \n",
    "    # 6. NLP-based features using spaCy\n",
    "    # Initialize columns for NLP features to ensure consistent shape\n",
    "    nlp_feature_columns = ['VERB', 'NOUN', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'NUM', \n",
    "                         'first_token_is_verb', 'has_imperative', 'sentence_count']\n",
    "    for col in nlp_feature_columns:\n",
    "        features[col] = 0\n",
    "    \n",
    "    # Process each text with spaCy\n",
    "    for idx, text in text_series.items():\n",
    "        try:\n",
    "            doc = nlp(text[:5000])  # Limit to 5000 chars to avoid memory issues\n",
    "            \n",
    "            # Count different parts of speech\n",
    "            pos_counts = {\n",
    "                'VERB': 0, 'NOUN': 0, 'ADJ': 0, 'ADV': 0, \n",
    "                'PRON': 0, 'DET': 0, 'ADP': 0, 'NUM': 0\n",
    "            }\n",
    "            \n",
    "            for token in doc:\n",
    "                if token.pos_ in pos_counts:\n",
    "                    pos_counts[token.pos_] += 1\n",
    "            \n",
    "            # Update feature dataframe with POS counts\n",
    "            for pos, count in pos_counts.items():\n",
    "                features.at[idx, pos] = count\n",
    "            \n",
    "            # Check if the first token is a verb (common in questions)\n",
    "            features.at[idx, 'first_token_is_verb'] = 1 if len(doc) > 0 and doc[0].pos_ == 'VERB' else 0\n",
    "            \n",
    "            # Check if there's an imperative verb (command) - common in summarization requests\n",
    "            has_imperative = 0\n",
    "            if len(doc) > 0 and doc[0].pos_ == 'VERB':\n",
    "                has_subject = any(token.dep_ == 'nsubj' for token in doc)\n",
    "                if not has_subject:\n",
    "                    has_imperative = 1\n",
    "            \n",
    "            features.at[idx, 'has_imperative'] = has_imperative\n",
    "            features.at[idx, 'sentence_count'] = len(list(doc.sents))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text at index {idx}: {str(e)}\")\n",
    "            # Keep default values (0) for this document\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text for bag-of-words features\n",
    "def vectorize_text(train_texts, test_texts):\n",
    "    # 1. Count Vectorizer (n-grams)\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "        max_features=500,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_train_counts = count_vectorizer.fit_transform(train_texts)\n",
    "    X_test_counts = count_vectorizer.transform(test_texts)\n",
    "    \n",
    "    # 2. TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=500,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(test_texts)\n",
    "    \n",
    "    return (X_train_counts, X_test_counts), (X_train_tfidf, X_test_tfidf), count_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance comparison\n",
    "def plot_model_comparison(results):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Bar chart of accuracy for each model\n",
    "    models = list(results.keys())\n",
    "    accuracy = [results[model]['accuracy'] for model in models]\n",
    "    \n",
    "    plt.bar(models, accuracy, color=['blue', 'green', 'red', 'purple'])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    \n",
    "    # Add accuracy values on top of each bar\n",
    "    for i, v in enumerate(accuracy):\n",
    "        plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(feature_names, importances, title='Feature Importance'):\n",
    "    # Get the top 20 features\n",
    "    indices = np.argsort(importances)[::-1][:20]\n",
    "    top_features = [feature_names[i] for i in indices]\n",
    "    top_importances = importances[indices]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(top_features)), top_importances, align='center')\n",
    "    plt.yticks(range(len(top_features)), top_features)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(json_file_path, test_size=0.2):\n",
    "    # Load the data\n",
    "    df = load_data(json_file_path)\n",
    "    \n",
    "    print(f\"Dataset loaded with {len(df)} samples\")\n",
    "    print(f\"Class distribution: {df['classification'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Check if 'classification' column exists\n",
    "    if 'classification' not in df.columns:\n",
    "        raise ValueError(\"The dataset doesn't have a 'classification' column\")\n",
    "    \n",
    "    # Check if 'input' column exists\n",
    "    if 'input' not in df.columns:\n",
    "        raise ValueError(\"The dataset doesn't have an 'input' column\")\n",
    "    \n",
    "    # Split the data into train and test sets - ensure indices are reset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['input'].reset_index(drop=True), \n",
    "        df['classification'].reset_index(drop=True), \n",
    "        test_size=test_size, \n",
    "        random_state=42,\n",
    "        stratify=df['classification'].reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    \n",
    "    # Verify X_train and X_test indices\n",
    "    print(f\"X_train index range: {X_train.index.min()} to {X_train.index.max()}\")\n",
    "    print(f\"X_test index range: {X_test.index.min()} to {X_test.index.max()}\")\n",
    "    \n",
    "    # Extract engineered features\n",
    "    print(\"Extracting features...\")\n",
    "    X_train_features = extract_features(X_train)\n",
    "    X_test_features = extract_features(X_test)\n",
    "\n",
    "    #print the features names\n",
    "    print(f\"Feature names: {X_train_features.columns.tolist()}\")\n",
    "    \n",
    "    # Verify dimensions of engineered features\n",
    "    print(f\"X_train_features shape: {X_train_features.shape}\")\n",
    "    print(f\"X_train row count: {len(X_train)}\")\n",
    "    \n",
    "    # Vectorize the text\n",
    "    print(\"Vectorizing text...\")\n",
    "    (X_train_counts, X_test_counts), (X_train_tfidf, X_test_tfidf), count_vectorizer, tfidf_vectorizer = vectorize_text(X_train, X_test)\n",
    "    \n",
    "    # Verify dimensions of vectorized text\n",
    "    print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
    "    \n",
    "    # Verify that dimensions match before concatenation\n",
    "    if len(X_train_features) != X_train_tfidf.shape[0]:\n",
    "        print(f\"WARNING: Dimension mismatch! X_train_features has {len(X_train_features)} rows, but X_train_tfidf has {X_train_tfidf.shape[0]} rows.\")\n",
    "        print(\"Adjusting X_train_features...\")\n",
    "        # Make sure they have same number of samples\n",
    "        if len(X_train_features) > X_train_tfidf.shape[0]:\n",
    "            X_train_features = X_train_features.iloc[:X_train_tfidf.shape[0]].copy()\n",
    "        else:\n",
    "            # This would be unusual, but handle it just in case\n",
    "            X_train_tfidf = X_train_tfidf[:len(X_train_features), :]\n",
    "            X_train = X_train.iloc[:len(X_train_features)]\n",
    "            y_train = y_train.iloc[:len(X_train_features)]\n",
    "    \n",
    "    # Same check for test data\n",
    "    if len(X_test_features) != X_test_tfidf.shape[0]:\n",
    "        print(f\"WARNING: Test dimension mismatch! X_test_features has {len(X_test_features)} rows, but X_test_tfidf has {X_test_tfidf.shape[0]} rows.\")\n",
    "        print(\"Adjusting X_test_features...\")\n",
    "        # Make sure they have same number of samples\n",
    "        if len(X_test_features) > X_test_tfidf.shape[0]:\n",
    "            X_test_features = X_test_features.iloc[:X_test_tfidf.shape[0]].copy()\n",
    "        else:\n",
    "            X_test_tfidf = X_test_tfidf[:len(X_test_features), :]\n",
    "            X_test = X_test.iloc[:len(X_test_features)]\n",
    "            y_test = y_test.iloc[:len(X_test_features)]\n",
    "    \n",
    "    # Final verification of dimensions\n",
    "    print(f\"Final X_train_features shape: {X_train_features.shape}\")\n",
    "    print(f\"Final X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
    "    print(f\"Final y_train length: {len(y_train)}\")\n",
    "    \n",
    "    # Convert sparse matrices to dense arrays for concatenation with other features\n",
    "    print(\"Combining features and vectorized text...\")\n",
    "    X_train_combined = np.hstack([\n",
    "        X_train_features.values,\n",
    "        X_train_tfidf.toarray()\n",
    "    ])\n",
    "    \n",
    "    X_test_combined = np.hstack([\n",
    "        X_test_features.values,\n",
    "        X_test_tfidf.toarray()\n",
    "    ])\n",
    "    \n",
    "    print(f\"X_train_combined shape: {X_train_combined.shape}\")\n",
    "    print(f\"X_test_combined shape: {X_test_combined.shape}\")\n",
    "    \n",
    "    # Train models and evaluate\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    results = {}\n",
    "    classes = sorted(list(df['classification'].unique()))\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.fit(X_train_combined, y_train)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train_combined, y_train, cv=5)\n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean CV score: {np.mean(cv_scores):.4f}\")\n",
    "        \n",
    "        # Predictions on test set\n",
    "        y_pred = model.predict(X_test_combined)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': cm,\n",
    "            'classes': classes\n",
    "        }\n",
    "        \n",
    "        # For ROC curve and precision-recall curve (if applicable)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test_combined)\n",
    "            \n",
    "            # Store probability predictions\n",
    "            results[name]['probabilities'] = y_prob\n",
    "            \n",
    "            # If binary classification\n",
    "            if len(classes) == 2:\n",
    "                # ROC curve\n",
    "                fpr, tpr, _ = roc_curve(y_test == classes[1], y_prob[:, 1])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                results[name]['fpr'] = fpr\n",
    "                results[name]['tpr'] = tpr\n",
    "                results[name]['auc'] = roc_auc\n",
    "                \n",
    "                # Precision-Recall curve\n",
    "                precision, recall, _ = precision_recall_curve(y_test == classes[1], y_prob[:, 1])\n",
    "                \n",
    "                results[name]['precision'] = precision\n",
    "                results[name]['recall'] = recall\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "    \n",
    "    best_model_name = [name for name, model in models.items() if model == best_model][0]\n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    joblib.dump(tfidf_vectorizer, \"vectorizer.pkl\")\n",
    "    print(\"Vectorizer saved to vectorizer.pkl\")\n",
    "    \n",
    "    # Feature importance analysis if it's a Random Forest\n",
    "    if isinstance(best_model, RandomForestClassifier):\n",
    "        feature_names = list(X_train_features.columns) + list(tfidf_vectorizer.get_feature_names_out())\n",
    "        importances = best_model.feature_importances_\n",
    "        \n",
    "        print(\"\\nTop 20 most important features:\")\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        for i in range(min(20, len(feature_names))):\n",
    "            print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plot_feature_importance(np.array(feature_names), importances)\n",
    "    \n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'model_name': best_model_name,\n",
    "        'accuracy': best_accuracy,\n",
    "        'feature_extractor': lambda texts: extract_features(pd.Series(texts)),\n",
    "        'vectorizer': tfidf_vectorizer,\n",
    "        'feature_names': list(X_train_features.columns) + list(tfidf_vectorizer.get_feature_names_out())\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, classifier_dict):\n",
    "    # Extract features\n",
    "    text_series = pd.Series([text])\n",
    "    features = extract_features(text_series)\n",
    "    vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "    \n",
    "    # Vectorize\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "\n",
    "    \n",
    "    # Combine features\n",
    "    X_combined = np.hstack([\n",
    "        features.values,\n",
    "        text_vectorized.toarray()\n",
    "    ])\n",
    "    \n",
    "    # load model classifier_model\n",
    "    model = joblib.load(\"classifier_model.pkl\")\n",
    "\n",
    "    prediction = model.predict(X_combined)[0]\n",
    "\n",
    "    confidence = None\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(X_combined)[0]\n",
    "        confidence = max(proba)\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'confidence': confidence\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier on qa_summarization_dataset.json...\n",
      "Dataset loaded with 505 samples\n",
      "Class distribution: {'Summarization': 255, 'QA': 250}\n",
      "Training set size: 404\n",
      "Test set size: 101\n",
      "X_train index range: 0 to 504\n",
      "X_test index range: 1 to 503\n",
      "Extracting features...\n",
      "Feature names: ['text_length', 'word_count', 'avg_word_length', 'has_question_mark', 'starts_with_what', 'starts_with_who', 'starts_with_when', 'starts_with_where', 'starts_with_why', 'starts_with_how', 'starts_with_which', 'starts_with_whose', 'starts_with_question_word', 'starts_with_can', 'starts_with_could', 'starts_with_would', 'starts_with_will', 'starts_with_should', 'starts_with_is', 'starts_with_are', 'starts_with_do', 'starts_with_does', 'starts_with_did', 'starts_with_question_verb', 'contains_summarization_word', 'VERB', 'NOUN', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'NUM', 'first_token_is_verb', 'has_imperative', 'sentence_count']\n",
      "X_train_features shape: (404, 36)\n",
      "X_train row count: 404\n",
      "Vectorizing text...\n",
      "X_train_tfidf shape: (404, 500)\n",
      "Final X_train_features shape: (404, 36)\n",
      "Final X_train_tfidf shape: (404, 500)\n",
      "Final y_train length: 404\n",
      "Combining features and vectorized text...\n",
      "X_train_combined shape: (404, 536)\n",
      "X_test_combined shape: (101, 536)\n",
      "\n",
      "Training Logistic Regression...\n",
      "Cross-validation scores: [0.97530864 0.98765432 1.         0.98765432 0.975     ]\n",
      "Mean CV score: 0.9851\n",
      "Logistic Regression Accuracy: 0.9604\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           QA       0.96      0.96      0.96        50\n",
      "Summarization       0.96      0.96      0.96        51\n",
      "\n",
      "     accuracy                           0.96       101\n",
      "    macro avg       0.96      0.96      0.96       101\n",
      " weighted avg       0.96      0.96      0.96       101\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  2]\n",
      " [ 2 49]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Training Naive Bayes...\n",
      "Cross-validation scores: [0.98765432 0.97530864 0.96296296 0.97530864 0.9875    ]\n",
      "Mean CV score: 0.9777\n",
      "Naive Bayes Accuracy: 0.9505\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           QA       0.98      0.92      0.95        50\n",
      "Summarization       0.93      0.98      0.95        51\n",
      "\n",
      "     accuracy                           0.95       101\n",
      "    macro avg       0.95      0.95      0.95       101\n",
      " weighted avg       0.95      0.95      0.95       101\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  4]\n",
      " [ 1 50]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Training SVM...\n",
      "Cross-validation scores: [0.98765432 0.98765432 1.         1.         1.        ]\n",
      "Mean CV score: 0.9951\n",
      "SVM Accuracy: 0.9802\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           QA       0.96      1.00      0.98        50\n",
      "Summarization       1.00      0.96      0.98        51\n",
      "\n",
      "     accuracy                           0.98       101\n",
      "    macro avg       0.98      0.98      0.98       101\n",
      " weighted avg       0.98      0.98      0.98       101\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [ 2 49]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Training Random Forest...\n",
      "Cross-validation scores: [0.97530864 1.         0.98765432 0.98765432 1.        ]\n",
      "Mean CV score: 0.9901\n",
      "Random Forest Accuracy: 0.9901\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           QA       0.98      1.00      0.99        50\n",
      "Summarization       1.00      0.98      0.99        51\n",
      "\n",
      "     accuracy                           0.99       101\n",
      "    macro avg       0.99      0.99      0.99       101\n",
      " weighted avg       0.99      0.99      0.99       101\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [ 1 50]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Best model: Random Forest\n",
      "Best accuracy: 0.9901\n",
      "Vectorizer saved to vectorizer.pkl\n",
      "\n",
      "Top 20 most important features:\n",
      "contains_summarization_word: 0.1886\n",
      "has_question_mark: 0.0692\n",
      "summary: 0.0477\n",
      "starts_with_question_word: 0.0465\n",
      "DET: 0.0417\n",
      "text_length: 0.0411\n",
      "NOUN: 0.0361\n",
      "ADP: 0.0351\n",
      "summarize: 0.0294\n",
      "avg_word_length: 0.0236\n",
      "starts_with_what: 0.0222\n",
      "VERB: 0.0201\n",
      "word_count: 0.0182\n",
      "ADJ: 0.0178\n",
      "starts_with_could: 0.0148\n",
      "main: 0.0144\n",
      "has_imperative: 0.0123\n",
      "need: 0.0118\n",
      "first_token_is_verb: 0.0095\n",
      "distill main: 0.0092\n",
      "Model saved to classifier_model.pkl\n",
      "\n",
      "Testing with examples:\n",
      "Text: What is the capital of France?\n",
      "Prediction: QA\n",
      "Confidence: 0.9800\n",
      "\n",
      "Text: Can you summarize the key points from this article?\n",
      "Prediction: Summarization\n",
      "Confidence: 0.8500\n",
      "\n",
      "Text: Explain how photosynthesis works\n",
      "Prediction: QA\n",
      "Confidence: 0.6200\n",
      "\n",
      "Text: Create a brief summary of the meeting minutes\n",
      "Prediction: Summarization\n",
      "Confidence: 0.9900\n",
      "\n",
      "Text: What are the benefits of regular exercise?\n",
      "Prediction: QA\n",
      "Confidence: 0.9900\n",
      "\n",
      "Text: Would you mind creating a digest of these research findings?\n",
      "Prediction: Summarization\n",
      "Confidence: 0.7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the classifier\n",
    "    json_file_path = 'qa_summarization_dataset.json'\n",
    "    print(f\"Training classifier on {json_file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        classifier_dict = train_classifier(json_file_path)\n",
    "\n",
    "        # save the classifier_dict to a JSON file\n",
    "        model = classifier_dict.pop(\"model\", None)  # Remove the model from the dictionary\n",
    "        if model:\n",
    "            joblib.dump(model, \"classifier_model.pkl\")  # Save the model to a .pkl file\n",
    "            print(\"Model saved to classifier_model.pkl\")\n",
    "   \n",
    "        print(\"\\nTesting with examples:\")\n",
    "        test_examples = [\n",
    "            \"What is the capital of France?\",\n",
    "            \"Can you summarize the key points from this article?\",\n",
    "            \"Explain how photosynthesis works\",\n",
    "            \"Create a brief summary of the meeting minutes\",\n",
    "            \"What are the benefits of regular exercise?\",\n",
    "            \"Would you mind creating a digest of these research findings?\"\n",
    "        ]\n",
    "        \n",
    "        for example in test_examples:\n",
    "            prediction = predict(example, classifier_dict)\n",
    "            print(f\"Text: {example}\")\n",
    "            print(f\"Prediction: {prediction['prediction']}\")\n",
    "            if prediction['confidence']:\n",
    "                print(f\"Confidence: {prediction['confidence']:.4f}\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: the recent news of today\n",
      "Prediction: Summarization\n"
     ]
    }
   ],
   "source": [
    "# write an example and pridict it is class \n",
    "example = \"the recent news of today\"\n",
    "prediction = predict(example, classifier_dict) \n",
    "print(f\"Text: {example}\")\n",
    "print(f\"Prediction: {prediction['prediction']}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
