{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-15T01:29:27.693578Z",
     "iopub.status.busy": "2025-03-15T01:29:27.693123Z",
     "iopub.status.idle": "2025-03-15T01:29:27.702725Z",
     "shell.execute_reply": "2025-03-15T01:29:27.701865Z",
     "shell.execute_reply.started": "2025-03-15T01:29:27.693543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertModel  # Changed to Fast tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Define constants\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "BERT_MODEL = 'bert-base-cased'  # Using cased variant as NER is case-sensitive\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# CoNLL-2003 has these entity types\n",
    "tag2idx = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1, 'I-PER': 2,\n",
    "    'B-ORG': 3, 'I-ORG': 4,\n",
    "    'B-LOC': 5, 'I-LOC': 6,\n",
    "    'B-MISC': 7, 'I-MISC': 8\n",
    "}\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, texts, tags, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = self.texts[idx]  # Already a list of words\n",
    "        tag_list = self.tags[idx]\n",
    "        \n",
    "        # For debugging\n",
    "        if idx == 0:\n",
    "            print(f\"Processing text: {' '.join(words[:10])}...\")\n",
    "            print(f\"Words length: {len(words)}, Tags length: {len(tag_list)}\")\n",
    "        \n",
    "        # Tokenize input text - words are already split\n",
    "        encoding = self.tokenizer(\n",
    "            words,  # Already a list of words, no need for split()\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get word_ids for alignment\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        \n",
    "        # Align tags with wordpiece tokens\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have word_id set to None\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(-100)  # -100 is ignored by PyTorch loss function\n",
    "            # First token of a word\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(tag_list):\n",
    "                    aligned_labels.append(tag_list[word_idx])\n",
    "                else:\n",
    "                    # Handle case where word_idx is out of range\n",
    "                    aligned_labels.append(0)  # Default to 'O'\n",
    "            # Subsequent subword tokens - use same label as first token\n",
    "            else:\n",
    "                if word_idx < len(tag_list):\n",
    "                    tag_val = tag_list[word_idx]\n",
    "                    # For B- tags, convert to I- for subwords\n",
    "                    if tag_val % 2 == 1 and tag_val > 0:  # if B- tag (odd value)\n",
    "                        aligned_labels.append(tag_val + 1)  # Convert to I- tag\n",
    "                    else:\n",
    "                        aligned_labels.append(tag_val)\n",
    "                else:\n",
    "                    aligned_labels.append(0)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(aligned_labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:29:28.424794Z",
     "iopub.status.busy": "2025-03-15T01:29:28.424534Z",
     "iopub.status.idle": "2025-03-15T01:29:28.431611Z",
     "shell.execute_reply": "2025-03-15T01:29:28.430907Z",
     "shell.execute_reply.started": "2025-03-15T01:29:28.424774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_conll2003():\n",
    "    # Use the actual path to your CoNLL-2003 data\n",
    "    file_path = \"/kaggle/input/datasetnernew/eng.train\"\n",
    "    sentences = []\n",
    "    tags_list = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence = []\n",
    "            sentence_tags = []\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Empty line indicates end of sentence\n",
    "                if not line:\n",
    "                    if sentence:\n",
    "                        sentences.append([word.lower() for word in sentence])   # Make a copy\n",
    "                        tags_list.append(sentence_tags.copy())  # Make a copy\n",
    "                        sentence.clear()\n",
    "                        sentence_tags.clear()\n",
    "                    continue\n",
    "                \n",
    "                # Skip document separator\n",
    "                if line.startswith('-DOCSTART-'):\n",
    "                    continue\n",
    "                \n",
    "                # Parse the token and its NER tag\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:  # CoNLL-2003 has token, POS, chunk, NER format\n",
    "                    token = parts[0]\n",
    "                    ner_tag = parts[3]\n",
    "                    \n",
    "                    sentence.append(token)\n",
    "                    sentence_tags.append(tag2idx.get(ner_tag, 0))  # Default to 'O' if tag not found\n",
    "                \n",
    "                # Print some sample data for debugging\n",
    "                if len(sentences) == 0 and len(sentence) < 10:\n",
    "                    print(f\"Sample token: {token}, tag: {ner_tag}\")\n",
    "            \n",
    "            # Add the last sentence if the file doesn't end with an empty line\n",
    "            if sentence:\n",
    "                sentences.append(sentence.copy())\n",
    "                tags_list.append(sentence_tags.copy())\n",
    "                \n",
    "        print(f\"Loaded {len(sentences)} sentences\")\n",
    "        if len(sentences) > 0:\n",
    "            print(f\"First sentence has {len(sentences[0])} tokens\")\n",
    "            \n",
    "        return sentences, tags_list\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        # Return empty lists in case of error\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:29:29.180722Z",
     "iopub.status.busy": "2025-03-15T01:29:29.180450Z",
     "iopub.status.idle": "2025-03-15T01:29:29.187032Z",
     "shell.execute_reply": "2025-03-15T01:29:29.186355Z",
     "shell.execute_reply.started": "2025-03-15T01:29:29.180702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERTSeq2SeqForNER(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(BERTSeq2SeqForNER, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Encoder (BERT)\n",
    "        # The BERT model will encode the input text\n",
    "        \n",
    "        # Decoder\n",
    "        # Simple linear layer for token classification\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "        # Additional seq2seq components\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.bert.config.hidden_size,\n",
    "            hidden_size=self.bert.config.hidden_size // 2,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get BERT embeddings\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # Process through LSTM\n",
    "        lstm_output, _ = self.lstm(sequence_output)\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        \n",
    "        # Apply classifier\n",
    "        logits = self.classifier(lstm_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, self.num_labels)\n",
    "            active_labels = torch.where(\n",
    "                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "            )\n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:29:30.507930Z",
     "iopub.status.busy": "2025-03-15T01:29:30.507618Z",
     "iopub.status.idle": "2025-03-15T01:29:30.519572Z",
     "shell.execute_reply": "2025-03-15T01:29:30.518775Z",
     "shell.execute_reply.started": "2025-03-15T01:29:30.507906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Load dataset\n",
    "    print(\"Loading CoNLL-2003 dataset...\")\n",
    "    sentences, tags_list = load_conll2003()\n",
    "    \n",
    "    if not sentences:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Split data\n",
    "    train_sentences, val_sentences, train_tags, val_tags = train_test_split(sentences, tags_list, test_size=0.1)\n",
    "    \n",
    "    print(f\"Train set: {len(train_sentences)} sentences\")\n",
    "    print(f\"Validation set: {len(val_sentences)} sentences\")\n",
    "    \n",
    "    # Initialize tokenizer - Use Fast version\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CoNLLDataset(train_sentences, train_tags, tokenizer, MAX_LEN)\n",
    "    val_dataset = CoNLLDataset(val_sentences, val_tags, tokenizer, MAX_LEN)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTSeq2SeqForNER(BERT_MODEL, len(tag2idx))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=total_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}'):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Average training loss: {avg_train_loss}')\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs['loss']\n",
    "                logits = outputs['logits']\n",
    "                \n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                # Convert logits to predictions\n",
    "                preds = torch.argmax(logits, dim=2)\n",
    "                \n",
    "                # Move predictions and labels to CPU\n",
    "                preds = preds.detach().cpu().numpy()\n",
    "                labels_np = labels.detach().cpu().numpy()\n",
    "                \n",
    "                # Store predictions and true labels\n",
    "                for i, p in enumerate(preds):\n",
    "                    label = labels_np[i]\n",
    "                    mask = label != -100\n",
    "                    predictions.append(p[mask])\n",
    "                    true_labels.append(label[mask])\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = eval_loss / len(val_loader)\n",
    "        print(f'Validation loss: {avg_val_loss}')\n",
    "        \n",
    "        # Calculate and print metrics\n",
    "        flat_predictions = [p for sublist in predictions for p in sublist]\n",
    "        flat_true_labels = [l for sublist in true_labels for l in sublist]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = sum(p == t for p, t in zip(flat_predictions, flat_true_labels)) / len(flat_true_labels)\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'bert_seq2seq_ner.pt')\n",
    "    print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:29:32.043848Z",
     "iopub.status.busy": "2025-03-15T01:29:32.043530Z",
     "iopub.status.idle": "2025-03-15T01:29:32.050679Z",
     "shell.execute_reply": "2025-03-15T01:29:32.049884Z",
     "shell.execute_reply.started": "2025-03-15T01:29:32.043824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # Load the model\n",
    "    model = BERTSeq2SeqForNER(BERT_MODEL, len(tag2idx))\n",
    "    model.load_state_dict(torch.load('bert_seq2seq_ner.pt'))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize tokenizer - Use Fast version\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL)\n",
    "    \n",
    "    # Tokenize input\n",
    "    words = text.lower().split()\n",
    "    inputs = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = inputs['input_ids'].to(DEVICE)\n",
    "    attention_mask = inputs['attention_mask'].to(DEVICE)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs['logits']\n",
    "        predictions = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
    "    \n",
    "    # Get word to token mapping\n",
    "    word_ids = inputs.word_ids(batch_index=0)\n",
    "    \n",
    "    # Map predictions to words\n",
    "    word_predictions = []\n",
    "    prev_word_idx = None\n",
    "    \n",
    "    for token_idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        \n",
    "        word = words[word_idx]\n",
    "        tag_idx = predictions[token_idx]\n",
    "        tag = idx2tag.get(tag_idx, \"O\")\n",
    "        \n",
    "        word_predictions.append((word, tag))\n",
    "        prev_word_idx = word_idx\n",
    "    \n",
    "    return word_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:29:37.342762Z",
     "iopub.status.busy": "2025-03-15T01:29:37.342458Z",
     "iopub.status.idle": "2025-03-15T01:58:23.805679Z",
     "shell.execute_reply": "2025-03-15T01:58:23.804913Z",
     "shell.execute_reply.started": "2025-03-15T01:29:37.342737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CoNLL-2003 dataset...\n",
      "Sample token: EU, tag: B-ORG\n",
      "Sample token: rejects, tag: O\n",
      "Sample token: German, tag: B-MISC\n",
      "Sample token: call, tag: O\n",
      "Sample token: to, tag: O\n",
      "Sample token: boycott, tag: O\n",
      "Sample token: British, tag: B-MISC\n",
      "Sample token: lamb, tag: O\n",
      "Sample token: ., tag: O\n",
      "Loaded 14041 sentences\n",
      "First sentence has 9 tokens\n",
      "Train set: 12636 sentences\n",
      "Validation set: 1405 sentences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d8f3579ea4435587d32d274292d492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78034cb3692e467fa35cd12dd3037f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014174f342f3484cba6efa4351b8e56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95b216fc66d4ed7af8e6dc55d4645ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3ddff136de400abfe3210437cd95cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  87%|████████▋ | 686/790 [04:42<00:43,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: thomas bjorn ( denmark ) , fernando roca ( spain...\n",
      "Words length: 13, Tags length: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 790/790 [05:26<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.4715768903305259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 1/88 [00:00<00:16,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: the most deflating double fault came when oncins was serving...\n",
      "Words length: 22, Tags length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 88/88 [00:12<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.13998553008687767\n",
      "Accuracy: 0.9624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  60%|█████▉    | 472/790 [03:18<02:13,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: thomas bjorn ( denmark ) , fernando roca ( spain...\n",
      "Words length: 13, Tags length: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 790/790 [05:32<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.10554427271473069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 1/88 [00:00<00:12,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: the most deflating double fault came when oncins was serving...\n",
      "Words length: 22, Tags length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 88/88 [00:12<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.09801412815101106\n",
      "Accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  15%|█▌        | 119/790 [00:50<04:44,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: thomas bjorn ( denmark ) , fernando roca ( spain...\n",
      "Words length: 13, Tags length: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 790/790 [05:31<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.055966960808521586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 1/88 [00:00<00:11,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: the most deflating double fault came when oncins was serving...\n",
      "Words length: 22, Tags length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 88/88 [00:12<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.08917587000178173\n",
      "Accuracy: 0.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  35%|███▍      | 276/790 [01:56<03:37,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: thomas bjorn ( denmark ) , fernando roca ( spain...\n",
      "Words length: 13, Tags length: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 790/790 [05:31<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.034365553230554145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 1/88 [00:00<00:12,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: the most deflating double fault came when oncins was serving...\n",
      "Words length: 22, Tags length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 88/88 [00:12<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.08490867409008471\n",
      "Accuracy: 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  32%|███▏      | 254/790 [01:46<03:45,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: thomas bjorn ( denmark ) , fernando roca ( spain...\n",
      "Words length: 13, Tags length: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 790/790 [05:31<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.024484963720381447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 1/88 [00:00<00:12,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: the most deflating double fault came when oncins was serving...\n",
      "Words length: 22, Tags length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 88/88 [00:12<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.08469250687366267\n",
      "Accuracy: 0.9801\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-d6cd3e4242e0>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('bert_seq2seq_ner.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple: B-ORG\n",
      "Inc.: I-ORG\n",
      "is: O\n",
      "planning: O\n",
      "to: O\n",
      "open: O\n",
      "a: O\n",
      "new: O\n",
      "store: O\n",
      "in: O\n",
      "Berlin,: O\n",
      "Germany: O\n",
      "next: O\n",
      "year.: O\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "    \n",
    "    # Example usage\n",
    "    sample_text = \"Apple Inc. is planning to open a new store in Berlin, Germany next year.\"\n",
    "    predictions = predict(sample_text)\n",
    "    \n",
    "    for word, tag in predictions:\n",
    "        print(f\"{word}: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6873315,
     "sourceId": 11035267,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
