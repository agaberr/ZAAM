{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1390040,"sourceType":"datasetVersion","datasetId":808105}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertModel, BertTokenizerFast, AdamW, get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nimport numpy as np\nfrom tqdm import tqdm\nimport random\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:09.866865Z","iopub.execute_input":"2025-04-01T16:51:09.867289Z","iopub.status.idle":"2025-04-01T16:51:30.225082Z","shell.execute_reply.started":"2025-04-01T16:51:09.867255Z","shell.execute_reply":"2025-04-01T16:51:30.224211Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def prepare_dataset(split=\"train\", sample_size=None):\n    print(f\"Loading MS MARCO dataset ({split} split)...\")\n    ds = load_dataset(\"microsoft/ms_marco\", \"v2.1\")\n    \n    dataset = ds[split]\n    \n    if sample_size:\n        # Select a random subset if sample_size is specified\n        indices = random.sample(range(len(dataset)), min(sample_size, len(dataset)))\n        dataset = dataset.select(indices)\n    \n    processed_examples = []\n    \n    for example in tqdm(dataset, desc=\"Processing examples\"):\n        #multiple passages use the one that contains the answer\n        question = example[\"query\"]\n        \n        # Skip examples without answers\n        if len(example[\"answers\"]) == 0 or not example[\"passages\"][\"is_selected\"]:\n            continue\n            \n        # Find the selected passage that contains the answer\n        selected_indices = [i for i, is_selected in enumerate(example[\"passages\"][\"is_selected\"]) \n                           if is_selected]\n        \n        if not selected_indices:\n            continue\n            \n        # Use the first selected passage as context\n        context_idx = selected_indices[0]\n        context = example[\"passages\"][\"passage_text\"][context_idx]\n        \n        # Get the answer\n        answer_text = example[\"answers\"][0]\n        \n        # Find answer position in the context\n        answer_start = context.find(answer_text)\n        \n        # Skip if answer not found in context\n        if answer_start == -1:\n            continue\n            \n        answer_end = answer_start + len(answer_text)\n        \n        processed_examples.append({\n            \"context\": context,\n            \"question\": question, \n            \"answer_text\": answer_text,\n            \"answer_start\": answer_start,\n            \"answer_end\": answer_end\n        })\n    \n    print(f\"Processed {len(processed_examples)} examples\")\n    return processed_examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:30.226226Z","iopub.execute_input":"2025-04-01T16:51:30.226850Z","iopub.status.idle":"2025-04-01T16:51:30.233198Z","shell.execute_reply.started":"2025-04-01T16:51:30.226817Z","shell.execute_reply":"2025-04-01T16:51:30.232365Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, examples, tokenizer, max_length=384):\n        self.examples = examples\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        \n        # Tokenize question and context\n        encoding = self.tokenizer(\n            example[\"question\"],\n            example[\"context\"],\n            max_length=self.max_length,\n            truncation=\"only_second\",\n            stride=128,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        # Get sequence IDs before converting to dictionary\n        sequence_ids = encoding.sequence_ids(0)\n        \n        # Get offsets and remove from encoding\n        offsets = encoding.pop(\"offset_mapping\")[0].tolist()\n        \n        # Remove batch dimension added by tokenizer for other tensors\n        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n        \n        # Find token positions for the answer\n        start_char = example[\"answer_start\"]\n        end_char = example[\"answer_end\"]\n        \n        # Find the tokens that correspond to the context (not the question)\n        context_start = 0\n        while sequence_ids[context_start] != 1:\n            context_start += 1\n            \n        # Find token start and end positions\n        token_start_position = token_end_position = 0\n        \n        for i, (offset_start, offset_end) in enumerate(offsets):\n            # Skip special tokens and question tokens\n            if sequence_ids[i] != 1:\n                continue\n                \n            # Check if this token contains the answer start\n            if offset_start <= start_char < offset_end:\n                token_start_position = i\n                \n            # Check if this token contains the answer end\n            if offset_start <= end_char <= offset_end:\n                token_end_position = i\n                break\n        \n        encoding[\"start_positions\"] = torch.tensor(token_start_position)\n        encoding[\"end_positions\"] = torch.tensor(token_end_position)\n        \n        return encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:32.662614Z","iopub.execute_input":"2025-04-01T16:51:32.663000Z","iopub.status.idle":"2025-04-01T16:51:32.673264Z","shell.execute_reply.started":"2025-04-01T16:51:32.662969Z","shell.execute_reply":"2025-04-01T16:51:32.672357Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class BertForQA(nn.Module):\n    def __init__(self, model_name=\"bert-base-uncased\"):\n        super(BertForQA, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.qa_outputs = nn.Linear(self.bert.config.hidden_size, 2)  # 2 for start/end position\n        \n    def forward(self, input_ids, attention_mask, token_type_ids=None, start_positions=None, end_positions=None):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        \n        sequence_output = outputs.last_hidden_state\n        logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        loss = None\n        if start_positions is not None and end_positions is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            loss = (start_loss + end_loss) / 2\n        \n        return {\n            \"loss\": loss,\n            \"start_logits\": start_logits,\n            \"end_logits\": end_logits\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:34.395672Z","iopub.execute_input":"2025-04-01T16:51:34.395936Z","iopub.status.idle":"2025-04-01T16:51:34.401600Z","shell.execute_reply.started":"2025-04-01T16:51:34.395916Z","shell.execute_reply":"2025-04-01T16:51:34.400861Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader=None, epochs=3, lr=5e-5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    optimizer = AdamW(model.parameters(), lr=lr)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0,\n        num_training_steps=total_steps\n    )\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        \n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        for batch in progress_bar:\n            # Move batch to device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Forward pass\n            outputs = model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                token_type_ids=batch.get(\"token_type_ids\", None),\n                start_positions=batch[\"start_positions\"],\n                end_positions=batch[\"end_positions\"]\n            )\n            \n            loss = outputs[\"loss\"]\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            train_loss += loss.item()\n            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n        \n        avg_train_loss = train_loss / len(train_dataloader)\n        print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n        \n        # Validation\n        if val_dataloader:\n            model.eval()\n            val_loss = 0\n            \n            with torch.no_grad():\n                for batch in tqdm(val_dataloader, desc=\"Validating\"):\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    \n                    outputs = model(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        token_type_ids=batch.get(\"token_type_ids\", None),\n                        start_positions=batch[\"start_positions\"],\n                        end_positions=batch[\"end_positions\"]\n                    )\n                    \n                    val_loss += outputs[\"loss\"].item()\n            \n            avg_val_loss = val_loss / len(val_dataloader)\n            print(f\"Validation loss: {avg_val_loss:.4f}\")\n    \n    return model\n\ndef answer_question(model, tokenizer, question, context, max_length=384):\n    \"\"\"Predict answer span from context\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    \n    # Tokenize input\n    inputs = tokenizer(\n        question, \n        context, \n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=128,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    # Get sequence IDs and offset mapping\n    sequence_ids = inputs.sequence_ids(0)\n    offset_mapping = inputs.pop(\"offset_mapping\").tolist()[0]\n    \n    # Move inputs to device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Forward pass\n    with torch.no_grad():\n        outputs = model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            token_type_ids=inputs.get(\"token_type_ids\", None)\n        )\n    \n    # Get predictions\n    start_logits = outputs[\"start_logits\"]\n    end_logits = outputs[\"end_logits\"]\n    \n    # Convert to Python lists\n    start_logits = start_logits[0].cpu().numpy()\n    end_logits = end_logits[0].cpu().numpy()\n    \n    # Get best answer (consider only context tokens)\n    context_tokens = []\n    for i, seq_id in enumerate(sequence_ids):\n        if seq_id == 1:  # 1 refers to context (not question or special tokens)\n            context_tokens.append(i)\n    \n    # Only consider answers in the context\n    start_logits = [float('-inf') if i not in context_tokens else score for i, score in enumerate(start_logits)]\n    end_logits = [float('-inf') if i not in context_tokens else score for i, score in enumerate(end_logits)]\n    \n    # Find best answer\n    start_idx = np.argmax(start_logits)\n    end_idx = np.argmax(end_logits[start_idx:]) + start_idx\n    \n    # Convert token indices to character spans\n    token_start, token_end = start_idx, end_idx\n    \n    # Get character span from token indices\n    char_start = offset_mapping[token_start][0]\n    char_end = offset_mapping[token_end][1]\n    \n    # Extract answer text\n    answer = context[char_start:char_end]\n    \n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:35.661312Z","iopub.execute_input":"2025-04-01T16:51:35.661561Z","iopub.status.idle":"2025-04-01T16:51:35.673221Z","shell.execute_reply.started":"2025-04-01T16:51:35.661542Z","shell.execute_reply":"2025-04-01T16:51:35.672416Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# Load and prepare dataset\ntrain_examples = prepare_dataset(split=\"train\", sample_size=100000)\nval_examples = prepare_dataset(split=\"validation\", sample_size=10000)\n\n# Initialize tokenizer - use the fast version explicitly\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# Create datasets\ntrain_dataset = QADataset(train_examples, tokenizer)\nval_dataset = QADataset(val_examples, tokenizer)\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16)\n\n# Initialize model\nmodel = BertForQA()\n\n# Train model\ntrained_model = train_model(\n    model, \n    train_dataloader, \n    val_dataloader, \n    epochs=3\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:38.600220Z","iopub.execute_input":"2025-04-01T16:51:38.600490Z","iopub.status.idle":"2025-04-01T18:20:14.613184Z","shell.execute_reply.started":"2025-04-01T16:51:38.600471Z","shell.execute_reply":"2025-04-01T18:20:14.612243Z"}},"outputs":[{"name":"stdout","text":"Loading MS MARCO dataset (train split)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3222dca5bc04d73804fe3b4df7517ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/210M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1415363add2d4adda70e4946f078d51f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00007.parquet:   0%|          | 0.00/240M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071c6c77308641c1b6325e981043a279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00007.parquet:   0%|          | 0.00/240M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0c2aa5bf5f4cf0b3b87ba555e4517e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00007.parquet:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"335f4c7bebec4996bcced481865b67fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00007.parquet:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e54523ec9c3f44babe64ff144175a0f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00007.parquet:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97bb7e70cd694071970686633cb629a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00007.parquet:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dad15935b0774d98bb1878cd41cb0575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00007.parquet:   0%|          | 0.00/244M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de180402bfef4206b48cb4d740f927a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/204M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d7224d5f0e94125bf2819d59f6325bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/101093 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed93c055e614dd8883839df648a61e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/808731 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"310ec41fac2549ee9cc475539b86423c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/101092 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae425286753f4fe194872d3b23679e1a"}},"metadata":{}},{"name":"stderr","text":"Processing examples: 100%|██████████| 100000/100000 [00:16<00:00, 5960.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processed 22788 examples\nLoading MS MARCO dataset (validation split)...\n","output_type":"stream"},{"name":"stderr","text":"Processing examples: 100%|██████████| 10000/10000 [00:01<00:00, 6465.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processed 1463 examples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22e3a7d493c24d3bb7d7bdc1d23c2002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f35255aef7d24a168ade0b210a16a350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f56e377ce644488f81ceacb296faba79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2d6b6f380c44314aea1c74d5c16fe03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae8f3dfc32ae4cc3a4e19d36dfea370a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1/3: 100%|██████████| 1425/1425 [28:35<00:00,  1.20s/it, loss=1.0407]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average training loss: 1.4859\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 92/92 [00:36<00:00,  2.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 1.0892\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 1425/1425 [28:39<00:00,  1.21s/it, loss=0.2553]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Average training loss: 0.8994\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 92/92 [00:36<00:00,  2.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 1.0985\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 1425/1425 [28:39<00:00,  1.21s/it, loss=0.0646]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Average training loss: 0.5442\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 92/92 [00:36<00:00,  2.51it/s]","output_type":"stream"},{"name":"stdout","text":"Validation loss: 1.2364\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Save model to current directory instead of a subdirectory\nmodel_path = \"./\"  # Current directory\ntorch.save(trained_model.state_dict(), f\"{model_path}pytorch_model.bin\")\ntokenizer.save_pretrained(model_path)\nprint(f\"Model saved to current directory\")\n\n# Example usage\ncontext = \"\"\"\nMohamed Salah is an Egyptian professional footballer who plays as a forward for Liverpool and the Egypt national team. Known for his incredible speed, dribbling, and finishing, he has won multiple Premier League and Champions League titles. Salah has broken numerous records, including becoming Liverpool’s all-time top scorer in the Champions League. He is a national hero in Egypt, inspiring millions with his achievements. His humility and dedication make him one of the greatest footballers of his generation.\n\"\"\"\nquestion = \"What is mohamed salah nationality?\"\n\nanswer = answer_question(trained_model, tokenizer, question, context)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:11.117641Z","iopub.execute_input":"2025-04-01T18:37:11.117944Z","iopub.status.idle":"2025-04-01T18:37:12.228594Z","shell.execute_reply.started":"2025-04-01T18:37:11.117921Z","shell.execute_reply":"2025-04-01T18:37:12.227833Z"}},"outputs":[{"name":"stdout","text":"Model saved to current directory\nQuestion: What is mohamed salah nationality?\nAnswer: Egyptian\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}